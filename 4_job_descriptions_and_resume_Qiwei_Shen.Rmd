---
title: "4_job_descriptions_and_resume.Rmd "
author  : "Qiwei Shen" 
date    : "2021-08-01" 
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: paper
    highlight: tango
---

## Libraries 
Load the following libraries, you may need to install a few!

library(tidyverse)
library(readxl)
library(tidytext)
library(wordcloud2)
library(janitor)

There is a bug in wordcloud2 which prevents it from knitting more than 1 wordcloud, so we want to install wordcloud2 using devtools from github using  devtools::install_github("gaospecial/wordcloud2"). Just do this once. if you don't have devtools installed, install it first then install wordcloud2 like this.

devtools::install_github("gaospecial/wordcloud2")

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(tidytext)
library(wordcloud2)
library(janitor)

```

# Setup 

-------- 

Part 1. Analyze Jobs 

## 1. Import 

1. Create jobs dataframe by using read_excel() to read in your MSBA30-50Jobs.xlsx (after you've loaded it with jobs), you can get started by using the MSBA20JobDescription.xlsx file if you want. 
2. I recommend piping it to clean_names() to deal with column names 
3. Print the first 5 jobs to make sure your import works. 

```{r, message=FALSE, warning=FALSE}
jobs <- read_xlsx('My20JobDescriptions.xlsx')%>%
  clean_names()

head(jobs,5)

```


# Task 1 – Job Description Term & Bigram Frequency Analysis 

--------

## Task 1.Create a table of term frequencies 

The following steps will help you make your  term_frequency table need to make your first wordcloud. 

0. make a vector called excludes <- this is just an exmaple c("key", "clients", "chicago")
1. pipe jobs into unnest_tokens(word, job_description)
2. pipe into anti_join(stop_words, by = c("word" = "word")) this will remove common words
3. pipe into filter(!word %in% excludes ) this will remove excluded words  
  - remove "key", "clients", "chicago" and any other words you think make sense
  
4. pipe into filter(!str_detect(word,"^\\d")) this will remove digits 
5. group_by(word) and summarize or use count()
6. arrange(desc(n)) 
7. create a term_frequency table 
8. finally, print out the the top 20 terms using top_n() or slice_max()


```{r}
excludes <- c("you'll","including","experience","requirements","required","preferred","responsibilities")
term_frequency<-jobs%>%
  unnest_tokens(word,job_description)%>%
  anti_join(stop_words, by = c("word" = "word"))%>%
  filter(!word %in% excludes )%>%
  filter(!str_detect(word,"^\\d"))%>%
  count(word,sort = T)
term_frequency%>%
  top_n(20)
```

### Task 1a. 

Create a wordcloud with all of the terms, using wordcloud2()

```{r}
term_frequency%>%
  wordcloud2()
```

### Task 1b. 

Create a wordcloud for the terms that start with the letter "a" 

```{r}
term_frequency%>%
  filter(str_sub(word,1,1)=='a')%>%
  wordcloud2()
```

### Task 1c. 

Create a word cloud of companies 

```{r}
jobs%>%
  count(firm,sort = T)%>%
  wordcloud2()
```


### Task 1d. 

Create a word cloud of job titles. 

```{r}
jobs%>%
  count(title,sort=T)%>%
  wordcloud2()
```



# Task 2, Words after DATA 

Clearly "DATA" an important word so what words come after data? 

------

## Task 2.1 - Words AFTER "data" ... 

1. pipe jobs 
2. into: unnest_tokens(bigram, job_description, token = "ngrams", n = 2, n_min = 2), what does bigram do?
3. separate bigram into two words: separate(bigram, c("word1", "word2"), sep = " ")
4. filter for "data" filter(word1 == "data") 
5. remove junk words on word2, filter(!word2 %in% excludes ) this will remove excluded word
6. unite word 1 and 2 together into a bigram
7. group_by(bigram)
8. summarize(n=n())
9. arrange(desc(n))
10. save data_term_frequency 

11. print top 10 data + terms 

```{r}
data_term_frequency <- jobs%>%
  unnest_tokens(bigram, job_description, token = "ngrams", n = 2, n_min = 2)%>% #bigram is all the sequences of two adjacent elements from strings in 'tittle'
  separate(bigram, c("word1", "word2"), sep = " ")%>%
  filter(word1 == "data") %>%
  filter(!word2 %in% excludes )%>%
  anti_join(stop_words, by = c("word2" = "word"))%>%
  unite(bigram,word1,word2,sep=" ")%>%
  group_by(bigram)%>%
  summarise(n=n())%>%
  arrange(desc(n))

head(data_term_frequency,10)

  
```

## Task 2.2 - Create a word cloud of data + term combinations

```{r}
data_term_frequency%>%
  wordcloud2()
```

## Task 2.3 - Create a bar chart of the top 15, data + term combinations

```{r}
data_term_frequency%>%
  top_n(15,n)%>%
  ggplot(aes(x=reorder(bigram,n),y=n))+
  geom_bar(stat='identity')+
  coord_flip()
```

# Task 3, Technology Term Analysis  

Analyze Technology Terms 
--------

## Task 3 Instructions. 

Here is a list of important technology terms - you of course are free to add more. 
1. load them up. 
2. add any additional technology term or bigram that you think are useful. 

```{r}
technology_words <- c(
    "analytics", 
    "data",
    "analyze",
    "r", 
    "python", 
    "sql", 
    "excel", 
    "cloud",
    "aws",
    "azure",
    "ec2",
    "sas",
    "spss",
    "saas",
    "spark",
    "tensorflow",
    "sagemaker",
    "tableau",
    "hadoop",
    "pyspark",
    "h2o.ai",
    "spark", 
    "ai",
    "shiny",
    "dash",
    "pca",
    "k-means",
    "emr",
    "mapreduce",
    "nosql",
    "hive"
    )


technology_bigram <- c(
  "amazon web",
  "big data",
  "business analytics",
  "google cloud",
  "microsoft azure",
  "machine learning",
  "data science",
  "deep learning",
  "neural network",
  "neural networks",
  "neural nets",
  "random forests",
  "random forest",
  "elastic search",
  "map reduce",
  "artificial intelligence",
  "data visualization"
)


```


## Create Word & Bi-Gram Frequencies - Jobs 

1.	Filter the term_frequency table based on the technology_terms provided, tech_term_freq
2.	Filter the  bigram_frequency table based on technology_bigrams provided, tech_bigram_freq
3.	Smash the results together into technology_term_frequency using using bind_rows()

```{r}
tech_term_freq <- term_frequency%>%
  filter(word %in% technology_words)
bigram_frequency <- jobs%>%
  unnest_tokens(bigram,job_description,token="ngrams",n=2)%>%
  separate(bigram, c("word1", "word2"), sep = " ")%>%
  filter(!word1 %in% excludes )%>%
  filter(!word2 %in% excludes )%>%
  anti_join(stop_words, by = c("word1" = "word"))%>%
  anti_join(stop_words, by = c("word2" = "word"))%>%
  unite(bigram,word1,word2,sep=" ")%>%
  group_by(bigram)%>%
  summarise(n=n())%>%
  ungroup()%>%
  arrange(desc(n))
  
tech_bigram_freq<-bigram_frequency%>%
  filter(bigram %in% technology_bigram)

colnames(tech_bigram_freq)[1]='term'
colnames(tech_term_freq)[1]='term'
technology_term_frequency <- tech_bigram_freq%>%
  bind_rows(tech_term_freq)%>%
  arrange(-n)
technology_term_frequency
  
```

### Task 3a. Make a Bar Chart of Technolgy Terms 

```{r}
technology_term_frequency%>%
  ggplot(aes(x=reorder(term,n),y=n))+
  geom_bar(stat="identity")+
  coord_flip()
  
```

### Task 3b. Make a Wordcloud of Techology Terms 

```{r}
technology_term_frequency%>%
  wordcloud2()
```


# Task 4. Your Resume 

---------

## Task 4 - getting started 

1. save/print your resume as PDF, it needs to be PDF format for this to work.  
2. install pdftools and load the library(pdftools)
3. use pdf_text to read your resume into a table. for example here's how i read my resume:  

resume <- pdf_text("/Users/mikames/Downloads/Michael Ames Resume_CurrentJan2019.pdf")
  
> note: this makes a character vector of your resume, we need to tidy it up and get it into a table. 

```{r}
library(pdftools)
resume <- pdf_text("QiweiShen_resume_202107.pdf")
```
  
## Task 4.2 - Parse your resume, filter out the common words, digits, and any excludes 
Your resume is now a character vector, you'll need to perform the following 
1. as_tibble(resume) will convert it to a data frame 
2. parse text into words, using unnest_tokens(word, value)
3. remove words you want to exclude 
4. remove numbers
5. group by words and count the words up. to make a resume_word_freq table.  

```{r}
excludes=c(str_to_lower(month.name),
           "china","shanghai","canada","apac","nc","school","university","fudan"
                )
resume_word_freq<-resume%>%
  as_tibble(resume)%>%
  unnest_tokens(word,value)%>%
  anti_join(stop_words)%>%
  filter(!word %in% excludes )%>%
  filter(!str_detect(word,"^\\d"))%>%
  count(word,sort=T)
resume_word_freq
```

## Task 4.3 - Create a word cloud of the remaining words in your resume

```{r}
wordcloud2(resume_word_freq)
```

## Task 4.4 filter for technology_words and technology_bigrams 

to do this you'll need to Bigram Freq. your resume, hen filter for technology_bigrams 

###  Task 4.4 Steps: Bigram Freq. your resume, filter for technology_bigrams  

1. as_tibble(resume) pipe into 
2. unnest_tokens(word, value, token = "ngrams", n = 2, n_min = 2) to make bigrams 
3. filter(word %in% technology_bigram ) for specific bigrams 
4. group_by(word) 
5. summarize(n=n()) 
6.  arrange(desc(n)) 
7.  make a resume_bigram_freq


```{r}
resume_bigram_freq<-as_tibble(resume)%>%
  unnest_tokens(word, value, token = "ngrams", n = 2, n_min = 2)%>%
  filter(word %in% technology_bigram )%>%
  group_by(word)%>%
  summarize(n=n()) %>%
  arrange(desc(n)) 
resume_bigram_freq
  
```

### 4.4 finally SMASH resume_word_freq & resume_bigram_freq together 

1. use bind_rows() to do this. 


```{r}
#tech word in resume
resume_tech_word_freq <- as_tibble(resume)%>%
  unnest_tokens(word, value, token = "words")%>%
  filter(word %in% technology_words )%>%
  group_by(word)%>%
  summarize(n=n()) %>%
  arrange(desc(n)) 


#combine words and bigrams
resume_tech_freq <- resume_tech_word_freq%>%
  bind_rows(resume_bigram_freq)
resume_tech_freq

```

### TASK 4.4a - make a bar chart of combined technolgy term frequencies

```{r}
resume_tech_freq%>%
  ggplot(aes(x=reorder(word,n),y=n))+
  geom_bar(stat="identity")+
  coord_flip()
```


### TASK 4.4b - make a wordcloud of combined technolgy term frequencies

```{r}
resume_tech_freq%>%
  wordcloud2()
```

## Task 5 – Compare resume jobs. 

5.1	What terms do your resume and jobs data have in common? That is, compare your resume's terms to the terms found in the job descriptions. 
```{r}
resume_word_freq%>%
  inner_join(term_frequency,by='word',suffix=c("_resume","_job"))%>%
  arrange(-n_resume)

```

5.2	Based on the job's terms, what terms are missing from your resume? Make a table of terms missing from your resume but found in your job descriptions. 
```{r}
term_frequency%>%
  anti_join(resume_word_freq,by='word')%>%
  arrange(-n)
```

5.3	What tech-skills does your resume and jobs have in common?
```{r}
resume_tech_freq%>%
  inner_join(technology_term_frequency,by=c('word'='term'),suffix =c('_resume','_job') )%>%
  arrange(-n_resume)
```

5.4	Based on the job's tech-skills what skills are missing from your resume? 

```{r}
technology_term_frequency%>%
  anti_join(resume_tech_freq,by=c('term'='word'))%>%
  arrange(-n)
```


